{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras import layers \n",
    "from keras import models\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense, Activation, Dropout\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import random \n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0813 22:59:02.168571 4708677056 deprecation_wrapper.py:119] From /anaconda2/envs/model1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0813 22:59:02.182677 4708677056 deprecation_wrapper.py:119] From /anaconda2/envs/model1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0813 22:59:02.200592 4708677056 deprecation_wrapper.py:119] From /anaconda2/envs/model1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0813 22:59:02.260866 4708677056 deprecation_wrapper.py:119] From /anaconda2/envs/model1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0813 22:59:02.261538 4708677056 deprecation_wrapper.py:119] From /anaconda2/envs/model1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0813 22:59:02.262124 4708677056 deprecation_wrapper.py:119] From /anaconda2/envs/model1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0813 22:59:02.303954 4708677056 deprecation_wrapper.py:119] From /anaconda2/envs/model1/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0813 22:59:02.383390 4708677056 deprecation.py:323] From /anaconda2/envs/model1/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "with open('model_hough.json', 'r') as f:\n",
    "        model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model.load_weights('model_hough.h5')\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "predictions = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 124, 124, 4)       104       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 124, 124, 4)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 17, 17, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 13, 13, 2)         202       \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 13, 13, 2)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 2, 2, 2)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 18        \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 324\n",
      "Trainable params: 324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hough(image):\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
    "    circles = cv2.HoughCircles(img, cv2.HOUGH_GRADIENT, 1.2, 100, minRadius = 1, maxRadius = 100)\n",
    "    if circles is not None:\n",
    "        # convert the (x, y) coordinates and radius of the circles to integers\n",
    "        circles = np.round(circles[0, :]).astype(\"int\")\n",
    " \n",
    "        # loop over the (x, y) coordinates and radius of the circles\n",
    "        for (x, y, r) in circles:\n",
    "            #draw the circle in the output image, then draw a rectangle\n",
    "            # corresponding to the center of the circle\n",
    "            cv2.circle(img, (x, y), r, (255, 255, 255), -1)\n",
    "            cv2.rectangle(img, (x - 5, y - 5), (x + 5, y + 5), (0, 128, 255), -1)\n",
    "            \n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code reads the image and performs laplacian of gaussian computer vision algorithm. \n",
    "#The graph is also created as a part of the function. \n",
    "from PIL import Image\n",
    "\n",
    "def check_shape(image):\n",
    "    image = cv2.imread(image)\n",
    "    height, width, channels = image.shape \n",
    "\n",
    "    if (height*width > 330000000):\n",
    "        return large_classify(image, 15)\n",
    "    else:\n",
    "        gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        _,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
    "    \n",
    "        contours,hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnt = contours[0]\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "    \n",
    "        image = image[y:y+h,x:x+w]\n",
    "        return small_classify(image, 15)\n",
    "\n",
    "def splice_image(image, x, y):\n",
    "    test = image\n",
    "    my_slice = test[x:y, x:y]\n",
    "    return my_slice\n",
    "\n",
    "#Pick random number of positions in image and detects blobs \n",
    "def large_classify(image, num):\n",
    "    image = cv2.resize(image, dsize=(10000, 10000))\n",
    "    #height, width, channels = image.shape\n",
    "    placer = num - 1\n",
    "    X = 100\n",
    "    Y = 200\n",
    "    #image = rgb2gray(image)\n",
    "    for i in range(placer):\n",
    "        im = splice_image(image, X, Y)\n",
    "        im = LOG(im)\n",
    "        im = cv2.resize(im, dsize=(128, 128))\n",
    "        im = im.reshape(-1,128,128,1)\n",
    "        predictions.append(model.predict_classes(im))\n",
    "        X = X+100\n",
    "        Y = Y+100\n",
    "    return predictions\n",
    "\n",
    "def small_classify(image, num):\n",
    "    #height, width, channels = image.shape\n",
    "    placer = num - 1\n",
    "    X = 100\n",
    "    Y = 500\n",
    "    #image = rgb2gray(image)\n",
    "    for i in range(placer):\n",
    "        im = splice_image(image, X, Y)\n",
    "        im = LOG(im)\n",
    "        im = cv2.resize(im, dsize=(128, 128))\n",
    "        im = im.reshape(-1,128,128,1)\n",
    "        predictions.append(model.predict_classes(im))\n",
    "        X = X+400\n",
    "        Y = Y+400\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0.78\n",
    "y = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(0.78) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0db70731aaaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mclf_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclf_array\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mvanilla_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     bagging_clf = BaggingClassifier(clf, \n\u001b[1;32m     19\u001b[0m        max_samples=0.4, max_features=10, random_state=seed)\n",
      "\u001b[0;32m/anaconda2/envs/model1/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/model1/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \"\"\"\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/model1/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/model1/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/model1/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/model1/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 142\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Dask dataframes may not return numeric shape[0] value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(0.78) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "# Get some classifiers to evaluate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "seed = 1075\n",
    "np.random.seed(seed)\n",
    "# Create classifiers\n",
    "rf = RandomForestClassifier()\n",
    "et = ExtraTreesClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "svc = SVC()\n",
    "rg = RidgeClassifier()\n",
    "clf_array = [rf, et, knn, svc, rg]\n",
    "for clf in clf_array:\n",
    "    vanilla_scores = cross_val_score(clf, X, y, cv=10, n_jobs=-1)\n",
    "    bagging_clf = BaggingClassifier(clf, \n",
    "       max_samples=0.4, max_features=10, random_state=seed)\n",
    "    bagging_scores = cross_val_score(bagging_clf, X, y, cv=10, \n",
    "       n_jobs=-1)\n",
    "    \n",
    "    print(\"Mean of: {1:.3f}, std: (+/-) {2:.3f [{0}]\")  \n",
    "                       #.format(clf.__class__.__name__, \n",
    "                       #vanilla_scores.mean(), vanilla_scores.std())\n",
    "    print(\"Mean of: {1:.3f}, std: (+/-) {2:.3f} [Bagging {0}]\\n\")\n",
    "                       #.format(clf.__class__.__name__, \n",
    "                        #bagging_scores.mean(), bagging_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = 0\n",
    "total = 0\n",
    "for item in res:\n",
    "    if(item==1):\n",
    "        prob+=1\n",
    "    total+=1\n",
    "print(prob/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from keras.models import model_from_json\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import cv2 \n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import os\n",
      "import os.path\n",
      "import random \n",
      "\n",
      "\n",
      "def LOG(image):\n",
      "    image = np.array(image, dtype=np.uint8)\n",
      "    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
      "    circles = cv2.HoughCircles(img, cv2.HOUGH_GRADIENT, 1.2, 100, minRadius = 1, maxRadius = 100)\n",
      "    if circles is not None:\n",
      "        # convert the (x, y) coordinates and radius of the circles to integers\n",
      "        circles = np.round(circles[0, :]).astype(\"int\")\n",
      " \n",
      "        # loop over the (x, y) coordinates and radius of the circles\n",
      "        for (x, y, r) in circles:\n",
      "            #draw the circle in the output image, then draw a rectangle\n",
      "            # corresponding to the center of the circle\n",
      "            cv2.circle(img, (x, y), r, (255, 255, 255), -1)\n",
      "            cv2.rectangle(img, (x - 5, y - 5), (x + 5, y + 5), (0, 128, 255), -1)\n",
      "            \n",
      "    return img  \n",
      "\n",
      "#The following code reads the image and performs laplacian of gaussian computer vision algorithm. \n",
      "#The graph is also created as a part of the function. \n",
      "from PIL import Image\n",
      "\n",
      "def check_shape(image):\n",
      "    print(\"checkpoint\")\n",
      "    image = cv2.imread(image)\n",
      "    height, width, channels = image.shape \n",
      "\n",
      "    if (height*width > 330000000):\n",
      "        return large_classify(image, 15)\n",
      "    else:\n",
      "        gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
      "        _,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
      "    \n",
      "        contours,hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
      "        cnt = contours[0]\n",
      "        x,y,w,h = cv2.boundingRect(cnt)\n",
      "    \n",
      "        image = image[y:y+h,x:x+w]\n",
      "        return small_classify(image, 15)\n",
      "\n",
      "def splice_image(image, x, y):\n",
      "    test = image\n",
      "    my_slice = test[x:y, x:y]\n",
      "    return my_slice\n",
      "\n",
      "#Pick random number of positions in image and detects blobs \n",
      "def large_classify(image, num):\n",
      "    with open('model_hough.json', 'r') as f:\n",
      "        model = model_from_json(f.read())\n",
      "    print(\"checkpoint\")\n",
      "    # Load weights into the new model\n",
      "    model.load_weights('model_hough.h5')\n",
      "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "    print(\"checkpoint\")\n",
      "    predictions = []\n",
      "    image = cv2.resize(image, dsize=(10000, 10000))\n",
      "    #height, width, channels = image.shape\n",
      "    placer = num - 1\n",
      "    X = 100\n",
      "    Y = 200\n",
      "    #image = rgb2gray(image)\n",
      "    for i in range(placer):\n",
      "        im = splice_image(image, X, Y)\n",
      "        im = LOG(im)\n",
      "        im = cv2.resize(im, dsize=(128, 128))\n",
      "        im = im.reshape(-1, 128,128,1)\n",
      "        predictions.append(model.predict_classes(im))\n",
      "        X = X+100\n",
      "        Y = Y+100\n",
      "    return predictions \n",
      "\n",
      "def small_classify(image, num):\n",
      "    with open('model_hough.json', 'r') as f:\n",
      "        model = model_from_json(f.read())\n",
      "    print(\"checkpoint\")\n",
      "    # Load weights into the new model\n",
      "    model.load_weights('model_hough.h5')\n",
      "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "    print(\"checkpoint\")\n",
      "    predictions = []\n",
      "    #height, width, channels = image.shape\n",
      "    placer = num - 1\n",
      "    X = 100\n",
      "    Y = 500\n",
      "    #image = rgb2gray(image)\n",
      "    for i in range(placer):\n",
      "        im = splice_image(image, X, Y)\n",
      "        im = LOG(im)\n",
      "        im = cv2.resize(im, dsize=(128, 128))\n",
      "        im = im.reshape(-1, 128,128,1)\n",
      "        predictions.append(model.predict_classes(im))\n",
      "        X = X+400\n",
      "        Y = Y+400\n",
      "    return predictions \n",
      "\n",
      "def analyze_image(image):\n",
      "    res = check_shape(image)\n",
      "    prob = 0\n",
      "    total = 0\n",
      "    for item in res:\n",
      "        if(item==1):\n",
      "            prob+=1\n",
      "        total+=1\n",
      "    return(prob/total)matplotlib\n",
      "numpy\n",
      "pandas\n",
      "Pillow\n",
      "scikit-image\n",
      "Opencv-python\n",
      "globus_sdk\n",
      "keras\n",
      "tensorflow\n",
      "dlhub_sdk"
     ]
    }
   ],
   "source": [
    "!cat main.py\n",
    "!cat requirements.txt\n",
    "from main import analyze_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlhub_sdk.client import DLHubClient\n",
    "\n",
    "dl = DLHubClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_serv = dl.search_by_servable(servable_name=\"candle*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlhub_sdk.models.servables.python import PythonStaticMethodModel, PythonClassMethodModel\n",
    "from dlhub_sdk.utils.schemas import validate_against_dlhub_schema\n",
    "from dlhub_sdk.utils.types import compose_argument_block\n",
    "from fair_research_login import NativeClient\n",
    "from home_run.version import __version__\n",
    "from unittest import TestCase\n",
    "from tempfile import mkstemp\n",
    "from platform import system\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlhub_sdk.models.servables.keras import KerasModel\n",
    "import pickle as pkl\n",
    "import json\n",
    "import keras\n",
    "from home_run.python import PythonStaticMethodServable, PythonClassMethodServable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the metadata file\n",
    "model = PythonStaticMethodModel.create_model('main', 'analyze_image')\n",
    "model.set_title('SEM_CNN_Model')\n",
    "model.set_name('brainCNN')\n",
    "model.set_domains([\"general\",\"machinelearning\"])\n",
    "\n",
    "\n",
    "# Add provenance information\n",
    "model.set_authors([\"Koripelly, Aarthi\"], [\"University of Chicago\"])\n",
    "model.set_abstract(\"A function and CNN model to predict if SEM image is focused or blurry.\")\n",
    "\n",
    "#Add\n",
    "model.add_files('model_hough.h5')\n",
    "model.add_files('model_hough.json')\n",
    "model.add_files('main.py')\n",
    "model.add_files('requirements.txt')\n",
    "\n",
    "\n",
    "# Set the input type to file\n",
    "model.set_inputs('file', 'A file')\n",
    "model.set_outputs('string', 'Output')\n",
    "\n",
    "# Make the servable\n",
    "servable = PythonStaticMethodServable(**model.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login and get a token for Petrel HTTPS\n",
    "client = NativeClient(client_id='7414f0b4-7d05-4bb6-bb00-076fa3f17cf5')\n",
    "tokens = client.login(requested_scopes=['https://auth.globus.org/scopes/56ceac29-e98a-440a-a594-b41e7a084b62/all'])\n",
    "auth_token = tokens[\"petrel_https_server\"]['access_token']\n",
    "headers = {'Authorization': f'Bearer {auth_token}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "servable.run({'url': 'https://45a53408-c797-11e6-9c33-22000a1e3b52.e.globus.org/ryan/KasthuriSEM/DAT2_b_aligned_blurry/z_0412.tif',\n",
    "              'headers': headers})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datacite': {'creators': [{'givenName': 'Aarthi',\n",
       "    'familyName': 'Koripelly',\n",
       "    'affiliations': 'University of Chicago'}],\n",
       "  'titles': [{'title': 'SEM_CNN_Model'}],\n",
       "  'publisher': 'DLHub',\n",
       "  'publicationYear': '2019',\n",
       "  'identifier': {'identifier': '10.YET/UNASSIGNED', 'identifierType': 'DOI'},\n",
       "  'descriptions': [{'description': 'A function and CNN model to predict if SEM image is focused or blurry.',\n",
       "    'descriptionType': 'Abstract'}],\n",
       "  'fundingReferences': [],\n",
       "  'relatedIdentifiers': [],\n",
       "  'alternateIdentifiers': [],\n",
       "  'rightsList': [],\n",
       "  'resourceType': {'resourceTypeGeneral': 'InteractiveResource'}},\n",
       " 'dlhub': {'version': '0.8.0',\n",
       "  'domains': ['general', 'machinelearning'],\n",
       "  'visible_to': ['public'],\n",
       "  'name': 'brainCNN',\n",
       "  'files': {'other': ['model_hough.h5',\n",
       "    'model_hough.json',\n",
       "    'main.py',\n",
       "    'requirements.txt']},\n",
       "  'type': 'servable'},\n",
       " 'servable': {'methods': {'run': {'input': {'type': 'file',\n",
       "     'description': 'A file'},\n",
       "    'output': {'type': 'string', 'description': 'Output'},\n",
       "    'parameters': {},\n",
       "    'method_details': {'method_name': 'analyze_image',\n",
       "     'module': 'main',\n",
       "     'autobatch': False}}},\n",
       "  'shim': 'python.PythonStaticMethodServable',\n",
       "  'type': 'Python static method'}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = dl.publish_servable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b41edbc6-ff1f-4093-8501-dfdbd9033b10'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'datacite': {'creators': [{'affiliations': 'University of Chicago', 'familyName': 'Koripelly', 'givenName': 'Aarthi'}], 'descriptions': [{'description': 'A function and CNN model to predict if SEM image is focused or blurry.', 'descriptionType': 'Abstract'}], 'identifier': {'identifier': '10.YET/UNASSIGNED', 'identifierType': 'DOI'}, 'publicationYear': '2019', 'publisher': 'DLHub', 'relatedIdentifiers': [{'relatedIdentifier': 'globus:ckYbBYabceTR', 'relatedIdentifierType': 'Globus', 'relationType': 'IsDescribedBy'}], 'resourceType': {'resourceTypeGeneral': 'InteractiveResource'}, 'titles': [{'title': 'SEM_CNN_Model'}]}, 'dlhub': {'build_location': '/mnt/dlhub_ingest/ec63a0f9-9c6e-46a4-9d62-ee960050a100-1565124525', 'domains': ['general', 'machinelearning'], 'ecr_arn': 'arn:aws:ecr:us-east-1:039706667969:repository/ec63a0f9-9c6e-46a4-9d62-ee960050a100', 'ecr_uri': '039706667969.dkr.ecr.us-east-1.amazonaws.com/ec63a0f9-9c6e-46a4-9d62-ee960050a100', 'files': {'other': ['model_hough.h5', 'model_hough.json', 'main.py', 'requirements.txt']}, 'funcx_id': '2d0ec624-24b1-4d1a-a208-a2ad2120f3af', 'id': 'ec63a0f9-9c6e-46a4-9d62-ee960050a100', 'identifier': 'globus:ckYbBYabceTR', 'name': 'brainCNN', 'owner': 'None', 'publication_date': '1565124525373', 'shorthand_name': 'None/brainCNN', 'transfer_method': {'POST': 'file', 'path': '/mnt/tmp/servable.zip'}, 'type': 'servable', 'user_id': 'None', 'version': '0.8.0', 'visible_to': ['public']}, 'servable': {'methods': {'run': {'input': {'description': 'A file', 'type': 'file'}, 'method_details': {'autobatch': False, 'method_name': 'analyze_image', 'module': 'main'}, 'output': {'description': 'Output', 'type': 'string'}}}, 'shim': 'python.PythonStaticMethodServable', 'type': 'Python static method'}}, {'datacite': {'creators': [{'affiliations': 'University of Chicago', 'familyName': 'Koripelly', 'givenName': 'Aarthi'}], 'descriptions': [{'description': 'A function and CNN model to predict if SEM image is focused or blurry.', 'descriptionType': 'Abstract'}], 'identifier': {'identifier': '10.YET/UNASSIGNED', 'identifierType': 'DOI'}, 'publicationYear': '2019', 'publisher': 'DLHub', 'relatedIdentifiers': [{'relatedIdentifier': 'globus:2Ys2UDoxRkdr', 'relatedIdentifierType': 'Globus', 'relationType': 'IsDescribedBy'}], 'resourceType': {'resourceTypeGeneral': 'InteractiveResource'}, 'titles': [{'title': 'SEM_CNN_Model'}]}, 'dlhub': {'build_location': '/mnt/dlhub_ingest/7f2a67a4-8eb7-44c2-b604-310f6069d8c5-1565124764', 'domains': ['general', 'machinelearning'], 'ecr_arn': 'arn:aws:ecr:us-east-1:039706667969:repository/7f2a67a4-8eb7-44c2-b604-310f6069d8c5', 'ecr_uri': '039706667969.dkr.ecr.us-east-1.amazonaws.com/7f2a67a4-8eb7-44c2-b604-310f6069d8c5', 'files': {'other': ['model_hough.h5', 'model_hough.json', 'main.py', 'requirements.txt']}, 'funcx_id': 'e7aaed7a-fd73-4271-81a7-17418ea561b6', 'id': '7f2a67a4-8eb7-44c2-b604-310f6069d8c5', 'identifier': 'globus:2Ys2UDoxRkdr', 'name': 'brainCNN', 'owner': 'akoripelly_gmail', 'publication_date': '1565124764122', 'shorthand_name': 'akoripelly_gmail/brainCNN', 'transfer_method': {'POST': 'file', 'path': '/mnt/tmp/servable.zip'}, 'type': 'servable', 'user_id': '18', 'version': '0.8.0', 'visible_to': ['public']}, 'servable': {'methods': {'run': {'input': {'description': 'A file', 'type': 'file'}, 'method_details': {'autobatch': False, 'method_name': 'analyze_image', 'module': 'main'}, 'output': {'description': 'Output', 'type': 'string'}}}, 'shim': 'python.PythonStaticMethodServable', 'type': 'Python static method'}}]\n",
      "None/brainCNN\n"
     ]
    }
   ],
   "source": [
    "df_serv = dl.search_by_servable(servable_name='brainCNN')\n",
    "print(df_serv)\n",
    "servable_name = df_serv[0]['dlhub']['shorthand_name']\n",
    "print(servable_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other, publishing to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "servable_name = 'mschwarting_anl/BlurryBrain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res4 = dl.run(servable_name, {'url': 'https://45a53408-c797-11e6-9c33-22000a1e3b52.e.globus.org/ryan/KasthuriSEM/S_000_597029104/Tile_r1-c1_S_000_597029104.tif',\n",
    "              'headers': headers}, input_type='json', asynchronous=True)\n",
    "res4 = res4.result()\n",
    "print(res4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
