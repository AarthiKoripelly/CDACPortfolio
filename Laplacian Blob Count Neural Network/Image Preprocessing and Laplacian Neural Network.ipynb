{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import random \n",
    "\n",
    "from math import sqrt\n",
    "from skimage import data\n",
    "from skimage.feature import blob_log\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code reads the image and performs laplacian of gaussian computer vision algorithm. \n",
    "#The graph is also created as a part of the function. \n",
    "from PIL import Image\n",
    "\n",
    "def check_shape(image, num):\n",
    "    image = cv2.imread(image)\n",
    "    height, width, channels = image.shape \n",
    "\n",
    "    if (height*width > 330000000):\n",
    "        return large_classify(image, num)\n",
    "    else:\n",
    "        gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        _,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
    "    \n",
    "        contours,hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnt = contours[0]\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "    \n",
    "        image = image[y:y+h,x:x+w]\n",
    "        return small_classify(image, num)\n",
    "\n",
    "def splice_image(image, x, y):\n",
    "    test = image\n",
    "    my_slice = test[x:y, x:y]\n",
    "    return my_slice\n",
    "\n",
    "#Pick random number of positions in image and detects blobs \n",
    "def large_classify(image, num):\n",
    "    array = []\n",
    "    #image = cv2.imread(image)\n",
    "    height, width, channels = image.shape\n",
    "    if(width>15000):\n",
    "        image = cv2.resize(image, dsize=(10000, 10000), interpolation=cv2.INTER_LINEAR)\n",
    "    placer = num - 1\n",
    "    X = width/300\n",
    "    Xfirst = 0\n",
    "    Y = X+height/300\n",
    "    Yfirst = X\n",
    "    for i in range(placer):\n",
    "        x_pick = int(random.uniform(Xfirst, X))\n",
    "        y_pick = int(random.uniform(Yfirst, Y))\n",
    "        array.append(LOG(splice_image(image, x_pick, y_pick)))\n",
    "        Xfirst = X\n",
    "        Yfirst = Y\n",
    "        X = X + width/300\n",
    "        Y = X+height/300\n",
    "    return array\n",
    "\n",
    "def small_classify(image, num):\n",
    "    array = []\n",
    "    #image = cv2.imread(image)\n",
    "    height, width, channels = image.shape\n",
    "    placer = num - 1\n",
    "    X = width/50\n",
    "    Xfirst = 0\n",
    "    Y = X+height/50\n",
    "    Yfirst = X\n",
    "    for i in range(placer):\n",
    "        x_pick = int(random.uniform(Xfirst, X))\n",
    "        y_pick = int(random.uniform(Yfirst, Y))\n",
    "        array.append(LOG(splice_image(image, x_pick, y_pick)))\n",
    "        Xfirst = X\n",
    "        Yfirst = Y\n",
    "        X = X + width/50\n",
    "        Y = X+height/50\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def LOG(image): \n",
    "    #Laplacian: Highlights regions of rapid intensity change\n",
    "    blobsLOG = blob_log(rgb2gray(image), max_sigma=30, num_sigma=10, threshold=.1)\n",
    "\n",
    "    # Computing radii\n",
    "    blobsLOG[:, 2] = blobsLOG[:, 2] * sqrt(2)\n",
    "\n",
    "\n",
    "    #Setting plot\n",
    "    blobList = [blobsLOG]\n",
    "    colors = ['yellow']\n",
    "    titles = ['Laplacian of Gaussian']\n",
    "    sequence = zip(blobList, colors, titles)\n",
    "    countofCircles = 0\n",
    "    \n",
    "    for index, (blobs, color, title) in enumerate(sequence):\n",
    "        for blob in blobs:\n",
    "            countofCircles=countofCircles+1\n",
    "        \n",
    "    return countofCircles \n",
    "\n",
    "def decide(image):\n",
    "    count = LOGandDOG(image) \n",
    "    if (count > 3000):\n",
    "        return('The image is focused.')\n",
    "    else:\n",
    "        return('The image is not focused.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading in Images\n",
    "train_focus_path = \"Testing_Focus_Images/\"\n",
    "train_outfocus_path = \"Testing_Outfocus_Images/\"\n",
    "\n",
    "training_set = []\n",
    "labels = []\n",
    "\n",
    "for filename in os.listdir(train_outfocus_path):\n",
    "    print(filename)\n",
    "    data_set = []\n",
    "    imagepath = train_outfocus_path + filename\n",
    "    data_set = find_blobs_out(imagepath, 7)\n",
    "    training_set.append(data_set)\n",
    "    labels.append(0)\n",
    "\n",
    "for filename in os.listdir(train_focus_path):\n",
    "    print(filename)\n",
    "    data_set = []\n",
    "    imagepath = train_focus_path + filename\n",
    "    data_set = find_blobs_focus(imagepath, 7)\n",
    "    training_set.append(data_set)\n",
    "    labels.append(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network to Train Based on Blob Samples\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Small Neural Network\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(training_set)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=6, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the neural network\n",
    "model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the data to the training dataset\n",
    "model.fit(X_train,y_train, batch_size=10, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print model accuracy\n",
    "print(accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOGandDOG_Predict(image):\n",
    "    np = []\n",
    "    data_set = check_shape(image)\n",
    "    np.append(data_set)\n",
    "    preds = gnb.predict(np)\n",
    "    if (preds==1):\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def LOGandDOG_Predict(image, num):\n",
    "    np = []\n",
    "    data_set = check_shape(image, num)\n",
    "    np.append(data_set)\n",
    "    preds = 0\n",
    "    print(np)\n",
    "    for i in data_set:\n",
    "        if i > 1000:\n",
    "            preds = preds+1\n",
    "    if (preds > 2):\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(image, num):\n",
    "    return (LOGandDOG_Predict(image, num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagesize(image):\n",
    "    image = cv2.imread(image)\n",
    "    height, width, channels = image.shape \n",
    "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    _,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
    "    \n",
    "    contours,hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = contours[0]\n",
    "    x,y,w,h = cv2.boundingRect(cnt)\n",
    "    \n",
    "    image = image[y:y+h,x:x+w]\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    _,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
    "    \n",
    "    contours,hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = contours[0]\n",
    "    x,y,w,h = cv2.boundingRect(cnt)\n",
    "    \n",
    "    image = image[y:y+h,x:x+w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# Model reconstruction from JSON file\n",
    "with open('model.json', 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
